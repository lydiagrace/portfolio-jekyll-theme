---
layout: post
title: 'Racism in Modern Facial Recognition Software'
---

  There have been innumerous mistakes in software that have perpetuated racist judgements, like when Google Photos tagged two black friends as gorillas, which as Benjamin says “goes back for centuries” (110). The man who tweeted that out said later, “What kind of sample image data you collected that would result in this son?” (Jacky Alcine).  When companies started rolling out software that unlocks your tech products with face recognition, black people reported the camera being unable to recognize them. One example of this was the 2009 viral video “HP computers are racist” which was inexplicable evidence that an HP webcam followed “white Wanda” around when she moved, but didn’t recognize “black Desi” when he got in frame, and the camera didn’t move around at all for his face.  He said the famous words “I think my blackness is interfering” with the camera’s ability to follow him, words that frame his own blackness as the issue when he knows it is not.  
  In 2019, the ACLU tested Amazon’s facial recognition software called Rekognition (which may as well be called “Selektion”??), and during the test, the software made many major mistakes.  It falsely identified 28 US Congresspeople with people in mugshots. 40% of the incorrect matches were for people of color, even though they only make up 20% of Congress (Snow). It could not be more irresponsible to use machine learning to train a facial recognition system using mugshots. Mugshots are actually before someone is convicted, so sharing them publicly is also terribly problematic.  But training computers to understand that criminals look like the people American police arrest is terrifying.  It also clearly doesn’t work – and if your billion dollar company’s software doesn’t even kind of work in 2019, I don’t know what you are doing, but maybe you should just give up.  However, as Professor Kelly Gates says in “Can Computers Be Racist?,” it doesn’t matter if the software works: “it is important to understand that even if a perfectly accurate facial recognition system were possible—which it is not—it still could be used in ways that reproduce structural inequalities” (15). The use of facial recognition software by law enforcement is a mistake that, when – not if – the software is abused, it will have serious life consequences, and they will be placed unfairly on the black community to deal with. 
	Benjamin maintains throughout her book that it doesn’t have to be this way. Technology is “racist” because of choices that have been made – and that means different choices can be made.  As she says, “The technical capacity was always there, but social awareness and incentives to ensure fair representation online were lacking” (94). 


{% include image.html url="http://www.gratisography.com" image="projects/proj-4/bike.jpg" %}
